# core/strategy/ml_strategy.py
import pandas as pd
import numpy as np
import joblib
import logging
from typing import Optional, Dict, List, Any
from pathlib import Path
from datetime import datetime, timedelta
import streamlit as st

try:
    from .dl_models import (
        LSTMModelHandler, TransformerModelHandler, AlphaTransformerModelHandler,
        TENSORFLOW_AVAILABLE, PositionalEncoding, TransformerEncoderBlock
    )

    if TENSORFLOW_AVAILABLE:
        from tensorflow.keras.models import load_model

        # æ›´æ–°è‡ªå®šä¹‰å¯¹è±¡å­—å…¸
        CUSTOM_OBJECTS = {
            'PositionalEncoding': PositionalEncoding,
            'TransformerEncoderBlock': TransformerEncoderBlock,
        }
except ImportError:
    CUSTOM_OBJECTS = {}
# --- å¯¼å…¥ä¾èµ–ï¼Œåªè¿›è¡Œä¸€æ¬¡ ---
try:
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    from sklearn.utils.validation import check_is_fitted

    SKLEARN_AVAILABLE = True

except ImportError:
    class RandomForestClassifier:
        pass


    class StandardScaler:
        pass


    def train_test_split(*args, **kwargs):
        pass


    def accuracy_score(*args, **kwargs):
        return 0.0


    def check_is_fitted(estimator, attributes=None):
        raise ImportError("scikit-learn is not installed.")


    SKLEARN_AVAILABLE = False
    logging.warning("scikit-learn æœªå®‰è£…ï¼Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åŠŸèƒ½å°†å—é™ã€‚")

try:
    from .dl_models import LSTMModelHandler, TransformerModelHandler, AlphaTransformerModelHandler, TENSORFLOW_AVAILABLE

    if TENSORFLOW_AVAILABLE:
        from tensorflow.keras.models import load_model
except ImportError:
    class LSTMModelHandler:
        pass


    class TransformerModelHandler:
        pass


    class AlphaTransformerModelHandler:
        pass


    TENSORFLOW_AVAILABLE = False


    def load_model(filepath):
        pass


    logging.warning("dl_models.py æˆ– TensorFlow æœªæ‰¾åˆ°ï¼Œæ·±åº¦å­¦ä¹ åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

try:
    from core.analysis.text_feature_extractor import TextFeatureExtractor

    TEXT_EXTRACTOR_AVAILABLE = True
except ImportError:
    class TextFeatureExtractor:
        pass


    TEXT_EXTRACTOR_AVAILABLE = False
    logging.warning("TextFeatureExtractor æœªæ‰¾åˆ°ï¼Œæ–‡æœ¬ç‰¹å¾åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

logger = logging.getLogger(__name__)


# --- ç¼“å­˜å‡½æ•° ---
# ä¸´æ—¶ä¿®æ”¹ï¼šç›´æ¥æ³¨é‡Šæ‰ç¼“å­˜è£…é¥°å™¨
# @st.cache_resource(show_spinner="åŠ è½½æœºå™¨å­¦ä¹ æ¨¡å‹èµ„æº...", ttl=3600)
def load_ml_resource(file_path: Path) -> Optional[Any]:
    if not file_path.exists():
        logger.warning(f"èµ„æºæ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
        return None

    if file_path.suffix in ['.h5', '.keras']:
        if not TENSORFLOW_AVAILABLE:
            logger.error("TensorFlow/Keras ä¸å¯ç”¨ã€‚")
            return None
        try:
            if CUSTOM_OBJECTS:
                return load_model(file_path, custom_objects=CUSTOM_OBJECTS)
            else:
                return load_model(file_path)
        except Exception as e:
            logger.error(f"åŠ è½½ Keras æ¨¡å‹å¤±è´¥ {file_path}: {e}", exc_info=True)
            return None

    elif file_path.suffix == '.joblib':
        try:
            import joblib
            obj = joblib.load(file_path)
            logger.info(f"æˆåŠŸåŠ è½½ joblib æ–‡ä»¶: {file_path}")
            return obj
        except Exception as e:
            logger.error(f"åŠ è½½ joblib æ–‡ä»¶å¤±è´¥ {file_path}: {e}", exc_info=True)
            return None

    else:
        logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}")
        return None


def clear_load_ml_resource_cache():
    try:
        load_ml_resource.clear()
    except Exception as e:
        logger.warning(f"Failed to clear ml_resource cache: {e}")


class SklearnHandler:
    def __init__(self): self.model, self.scaler = None, None


class MLStrategy:
    def __init__(self, config: Dict, data_manager_ref: Optional[Any] = None):
        self.is_available = SKLEARN_AVAILABLE or TENSORFLOW_AVAILABLE
        if not self.is_available: logger.critical("CRITICAL: No ML framework available.")

        self.config = config
        self.data_manager = data_manager_ref
        self.active_model_handler: Optional[Any] = None
        self.current_model_name: Optional[str] = None
        self.current_model_filename: Optional[str] = None
        self.logger = logging.getLogger(__name__)
        self.models_base_path = getattr(config, 'ML_MODELS_BASE_PATH', Path("models"))

        self.base_feature_columns = list(getattr(config, 'BASE_FEATURE_COLUMNS', []))
        self.text_feature_columns = list(getattr(config, 'TEXT_FEATURE_COLUMNS', []))
        self.required_features = self.base_feature_columns[:]

        self.text_feature_extractor = None
        self.logger.info("Initializing TextFeatureExtractor within MLStrategy...")
        if TEXT_EXTRACTOR_AVAILABLE and self.data_manager:
            try:
                self.text_feature_extractor = TextFeatureExtractor(config, self.data_manager)
                if self.text_feature_extractor.is_available:
                    self.logger.info("TextFeatureExtractor initialized successfully and IS AVAILABLE.")
                else:
                    self.logger.warning(
                        "TextFeatureExtractor initialized, but IS NOT AVAILABLE. Check Gemini Key/Library.")
            except Exception as e:
                logger.error(f"Failed to initialize TextFeatureExtractor: {e}")
        else:
            # æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—
            self.logger.warning(
                f"Skipping TextFeatureExtractor init. TEXT_EXTRACTOR_AVAILABLE={TEXT_EXTRACTOR_AVAILABLE}, self.data_manager is not None={self.data_manager is not None}")

        if self.is_available:
            default_model_name = getattr(config, 'DEFAULT_ML_MODEL_NAME', None)
            if default_model_name and default_model_name != "æ— å¯ç”¨æ¨¡å‹":
                self.set_active_model(default_model_name)



    def _get_model_type(self, model_filename: str) -> str:
        if model_filename.endswith(('.h5', '.keras')):
            if 'transformer' in model_filename.lower(): return 'AlphaTransformer'
            return 'LSTM'
        elif model_filename.endswith('.joblib'):
            return 'RandomForest'
        return 'Unknown'

    def set_active_model(self, model_display_name: str) -> bool:
        if not self.is_available: return False
        self.logger.info(f"Attempting to set active model to: {model_display_name}")
        self.active_model_handler, self.current_model_name, self.current_model_filename = None, None, None
        available_models = getattr(self.config, 'AVAILABLE_ML_MODELS', {})
        if model_display_name not in available_models:
            self.logger.error(f"Model '{model_display_name}' not defined in config.");
            return False

        model_filename = available_models[model_display_name]
        model_type = self._get_model_type(model_filename)
        model_path = self.models_base_path / model_filename
        self.current_model_name, self.current_model_filename = model_display_name, model_filename

        if model_type in ['LSTM', 'AlphaTransformer']:
            if not TENSORFLOW_AVAILABLE: logger.error("TensorFlow not installed."); return False
            HandlerClass = LSTMModelHandler if model_type == 'LSTM' else AlphaTransformerModelHandler
            self.active_model_handler = HandlerClass(self.config)
            self.active_model_handler.model = load_ml_resource(model_path)
            if self.active_model_handler.model:
                logger.info(f"{model_type} model '{model_display_name}' loaded.")
            else:
                logger.warning(f"{model_type} model file '{model_filename}' not found. Needs training.")
        elif model_type == 'RandomForest':
            if not SKLEARN_AVAILABLE: logger.error("scikit-learn not installed."); return False
            self.active_model_handler = SklearnHandler()
            scaler_path = self.models_base_path / f"{Path(model_filename).stem}_scaler.joblib"
            self.active_model_handler.model = load_ml_resource(model_path)
            self.active_model_handler.scaler = load_ml_resource(scaler_path)
            if not self.active_model_handler.scaler: self.active_model_handler.scaler = StandardScaler()
        else:
            logger.error(f"Unknown model type '{model_type}' for file '{model_filename}'");
            return False
        return True

    def train(self, data: pd.DataFrame, symbol: str, model_display_name_to_save: Optional[str] = None) -> Dict:
        """
        [æ•°æ®æµä¿®å¤ç‰ˆ] è®­ç»ƒæ¨¡å‹ï¼Œç¡®ä¿ä¸é¢„æµ‹/å›æµ‹ä½¿ç”¨ç›¸åŒçš„æ•°æ®å¤„ç†æµç¨‹ã€‚
        """
        if not self.active_model_handler:
            if not model_display_name_to_save or not self.set_active_model(model_display_name_to_save):
                return {'success': False, 'message': f"æ— æ³•è®¾ç½®æˆ–åˆå§‹åŒ–æ¨¡å‹ '{model_display_name_to_save}'ã€‚"}

        model_type = self._get_model_type(self.current_model_filename)

        logger.info(f"å¼€å§‹è®­ç»ƒ {model_type} æ¨¡å‹: {model_display_name_to_save}")
        logger.info(f"åŸå§‹è®­ç»ƒæ•°æ®å½¢çŠ¶: {data.shape}")

        try:
            # 1. ä½¿ç”¨ç›¸åŒçš„ç‰¹å¾å‡†å¤‡æµç¨‹
            features_df = self.prepare_features(data, symbol=symbol)
            if features_df.empty:
                return {'success': False, 'message': "ç‰¹å¾å‡†å¤‡å¤±è´¥"}

            logger.info(f"ç‰¹å¾å‡†å¤‡åæ•°æ®å½¢çŠ¶: {features_df.shape}")

            # 2. è°ƒç”¨å¯¹åº”çš„è®­ç»ƒæ–¹æ³•
            if model_type in ['LSTM', 'AlphaTransformer']:
                return self._train_dl(features_df, symbol, model_type)
            elif model_type == 'RandomForest':
                return self._train_sklearn(features_df, symbol)
            else:
                return {'success': False, 'message': f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_type}'ã€‚"}

        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹å¤±è´¥: {e}", exc_info=True)
            return {'success': False, 'message': f"è®­ç»ƒå¤±è´¥: {str(e)}"}

    def predict(self, data: pd.DataFrame, symbol: str) -> Optional[Dict]:
        """
        [é‡æ„] é¢„æµ‹ã€‚è°ƒåº¦åˆ°å…·ä½“çš„é¢„æµ‹æ–¹æ³•ã€‚
        """
        if not self.active_model_handler or getattr(self.active_model_handler, 'model', None) is None:
            return {'message': f"æ¨¡å‹ '{self.current_model_name or 'æœªçŸ¥'}' æœªåŠ è½½ï¼Œæ— æ³•é¢„æµ‹ã€‚"}

        model_type = self._get_model_type(self.current_model_filename)

        if model_type in ['LSTM', 'AlphaTransformer']:
            return self._predict_dl(data, symbol, model_type)
        elif model_type == 'RandomForest':
            return self._predict_sklearn(data, symbol)
        else:
            return {'message': f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹ '{model_type}'ã€‚"}

    def _train_dl(self, data: pd.DataFrame, symbol: str, model_type: str) -> Dict:
        """[æœ€ç»ˆä¿®å¤ç‰ˆ] è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œé‡‡ç”¨æ›´å¥å£®çš„ NaN å¤„ç†å’Œæ•°æ®æµã€‚"""
        handler = self.active_model_handler
        if not handler: return {'success': False, 'message': f"DL å¤„ç†å™¨æœªå‡†å¤‡å¥½ç”¨äº {model_type}ã€‚"}
        params = self.config.ML_HYPERPARAMETERS.get(model_type, {})
        lookback = params.get('lookback', 60)
        self.logger.info(f"å¼€å§‹è®­ç»ƒ {model_type} æ¨¡å‹: {self.current_model_name}...")
        try:
            # 1. å‡†å¤‡æ‰€æœ‰ç‰¹å¾ (ç°åœ¨ä¼šè¿”å›ä¸€ä¸ªå¸¦æœ‰å¤§é‡ NaN çš„ DataFrame)
            features_df_full = self.prepare_features(data, symbol=symbol)
            if features_df_full.empty: return {'success': False, 'message': "ç‰¹å¾å‡†å¤‡è¿”å›äº†ç©ºçš„æ•°æ®æ¡†ã€‚"}

            # vvvvvvvvvvvvvvvvvvvv START OF FIX vvvvvvvvvvvvvvvvvvvv
            # 2. ç”Ÿæˆç›®æ ‡åˆ—
            if 'close' not in features_df_full.columns:
                return {'success': False, 'message': "prepare_features æœªèƒ½è¿”å› 'close' åˆ—ã€‚"}
            if model_type == 'AlphaTransformer':
                features_df_full['target'] = features_df_full['close'].pct_change(periods=5).shift(-5)
            else:
                features_df_full['target'] = (features_df_full['close'].shift(-1) > features_df_full['close']).astype(
                    int)

            # 3. æ›¿æ¢æ— ç©·å¤§å€¼
            features_df_full.replace([np.inf, -np.inf], np.nan, inplace=True)

            # 4. æ‰§è¡Œä¸€æ¬¡æ€§çš„ã€æœ€ç»ˆçš„ dropna
            # è¿™ä¼šç§»é™¤æ‰€æœ‰å› ä¸ºç‰¹å¾è®¡ç®—ï¼ˆå¼€å¤´ï¼‰å’Œç›®æ ‡è®¡ç®—ï¼ˆæœ«å°¾ï¼‰äº§ç”Ÿçš„æ— æ•ˆè¡Œ
            features_df_clean = features_df_full.dropna()
            # ^^^^^^^^^^^^^^^^^^^^ END OF FIX ^^^^^^^^^^^^^^^^^^^^

            self.logger.info(f"Data for training, after NaN cleanup: {features_df_clean.shape[0]} rows.")
            if len(features_df_clean) < lookback + 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ•°æ®ç”¨äº lookback å’Œ train/val split
                return {'success': False, 'message': f"æ¸…ç†åæ•°æ®ä¸¥é‡ä¸è¶³({len(features_df_clean)})ï¼Œæ— æ³•è®­ç»ƒã€‚"}

                # 5. ä»å¹²å‡€çš„ DataFrame ä¸­å‡†å¤‡ NumPy æ•°ç»„
            training_feature_cols = [col for col in self.required_features if col in features_df_clean.columns]
            X_unscaled = features_df_clean[training_feature_cols].values.astype(np.float32)
            y = features_df_clean['target'].values.astype(np.float32)

            # 6. è®­ç»ƒ/éªŒè¯åˆ†å‰² (åœ¨ NumPy æ•°ç»„ä¸Šè¿›è¡Œ)
            split_idx = int(len(X_unscaled) * 0.8)
            X_train_unscaled, X_val_unscaled = X_unscaled[:split_idx], X_unscaled[split_idx:]
            y_train, y_val = y[:split_idx], y[split_idx:]

            # 7. è®­ç»ƒå¹¶åº”ç”¨ Scaler
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train_unscaled)
            X_val_scaled = scaler.transform(X_val_unscaled)

            # 8. å°†å½’ä¸€åŒ–åçš„ NumPy æ•°ç»„é‡å¡‘ä¸º DL æ ¼å¼
            X_train_final, y_train_final = handler.prepare_data(X_train_scaled, y_train, lookback)
            X_val_final, y_val_final = handler.prepare_data(X_val_scaled, y_val, lookback)

            if len(X_train_final) == 0:
                return {'success': False, 'message': "é‡å¡‘åè®­ç»ƒæ ·æœ¬ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ lookback è®¾ç½®ã€‚"}

            # 9. æ„å»ºå’Œè®­ç»ƒæ¨¡å‹
            if handler.model is None:
                handler.build_model(input_shape=(X_train_final.shape[1], X_train_final.shape[2]))

            history = handler.train(X_train_final, y_train_final, X_val_final, y_val_final)

            # 10. ä¿å­˜æ¨¡å‹å’Œ Scaler
            model_path = self.models_base_path / self.current_model_filename
            scaler_path = self.models_base_path / f"{Path(self.current_model_filename).stem}_scaler.joblib"
            handler.model.save(model_path)
            if SKLEARN_AVAILABLE:  # ä¿å­˜ scaler çš„ feature names
                try:
                    scaler.feature_names_in_ = training_feature_cols
                except Exception:
                    pass
            joblib.dump(scaler, scaler_path)
            clear_load_ml_resource_cache()

            # 11. è¿”å›ç»“æœ
            metric_key = 'val_accuracy' if model_type != 'AlphaTransformer' else 'val_mean_absolute_error'
            final_val_metric = history.get(metric_key, [0])[-1]
            return {'success': True, 'message': f"{model_type} æ¨¡å‹ '{self.current_model_name}' è®­ç»ƒå®Œæˆã€‚",
                    'validation_metric': final_val_metric, 'n_samples': len(X_train_final), 'history': history}

        except Exception as e:
            self.logger.error(f"è®­ç»ƒ {model_type} æ¨¡å‹æ—¶å‡ºé”™: {e}", exc_info=True)
            return {'success': False, 'message': f"è®­ç»ƒ {model_type} å‡ºé”™: {e}"}

    def _reshape_for_dl(self, data: np.ndarray, lookback: int) -> np.ndarray:
        """[æ–°å¢] å°† 2D NumPy æ•°ç»„é‡å¡‘ä¸º DL æ¨¡å‹éœ€è¦çš„ 3D (samples, timesteps, features) æ ¼å¼"""
        if len(data) < lookback:
            return np.array([])  # è¿”å›ç©ºæ•°ç»„

        shape = (data.shape[0] - lookback + 1, lookback, data.shape[1])
        strides = (data.strides[0], data.strides[0], data.strides[1])
        return np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)

    def _train_sklearn(self, data: pd.DataFrame, symbol: str) -> Dict:
        """[ç§æœ‰] è®­ç»ƒ RandomForest æ¨¡å‹ (ä¿®å¤äº† NaN å¤„ç†)"""
        handler = self.active_model_handler
        self.logger.info(f"å¼€å§‹è®­ç»ƒ RandomForest æ¨¡å‹: {self.current_model_name}...")
        try:
            if handler.model is None:
                rf_params = self.config.ML_HYPERPARAMETERS.get('RandomForestClassifier', {})
                handler.model = RandomForestClassifier(**rf_params)
            if handler.scaler is None: handler.scaler = StandardScaler()

            features_df_full = self.prepare_features(data, symbol=symbol)
            if features_df_full.empty:
                return {'success': False, 'message': "ç‰¹å¾å‡†å¤‡è¿”å›äº†ç©ºçš„æ•°æ®æ¡†ã€‚"}

            # 2. ç”Ÿæˆæ ‡ç­¾
            labels = (features_df_full['close'].shift(-1) > features_df_full['close']).astype(int)
            features_df_full['labels'] = labels

            # 3. æ¸…ç† NaN
            features_df_clean = features_df_full.dropna()
            # ^^^^^^^^^^^^^^^^^^^^ END OF FIX ^^^^^^^^^^^^^^^^^^^^

            self.logger.info(f"Sklearn: Cleaned data for training: {features_df_clean.shape[0]} rows.")
            if features_df_clean.empty or len(features_df_clean) < 65:
                return {'success': False, 'message': f"ç‰¹å¾å‡†å¤‡å¤±è´¥æˆ–æ•°æ®ä¸è¶³ ({len(features_df_clean)}è¡Œ)ã€‚"}

                # 4. å‡†å¤‡ NumPy æ•°ç»„
            features_values = features_df_clean[self.required_features].values
            labels_values = features_df_clean['labels'].values

            split_idx = int(len(features_values) * 0.8)
            X_train, X_test = features_values[:split_idx], features_values[split_idx:]
            y_train, y_test = labels_values[:split_idx], labels_values[split_idx:]

            if len(np.unique(y_train)) < 2 or len(np.unique(y_test)) < 2:
                return {'success': False, 'message': "è®­ç»ƒé›†æˆ–æµ‹è¯•é›†ä¸­åªåŒ…å«å•ä¸€ç±»åˆ«ã€‚"}

            handler.scaler.fit(X_train)
            X_train_scaled = handler.scaler.transform(X_train)
            X_test_scaled = handler.scaler.transform(X_test)
            handler.model.fit(X_train_scaled, y_train)

            train_score = accuracy_score(y_train, handler.model.predict(X_train_scaled))
            test_score = accuracy_score(y_test, handler.model.predict(X_test_scaled))

            self._save_model()

            return {'success': True, 'train_score': train_score, 'test_score': test_score,
                    'n_samples': len(features_values),
                    'message': f"RandomForest æ¨¡å‹ '{self.current_model_name}' è®­ç»ƒå®Œæˆå¹¶ä¿å­˜ã€‚"}
        except Exception as e:
            self.logger.error(f"è®­ç»ƒ RandomForest æ¨¡å‹æ—¶å‡ºé”™: {e}", exc_info=True)
            return {'success': False, 'message': f"è®­ç»ƒå‡ºé”™: {e}"}


    def _predict_sklearn(self, data: pd.DataFrame, symbol: str) -> Optional[Dict]:
        """[ä¿®å¤ç‰ˆ] ä¸º Scikit-learn æ¨¡å‹æ‰§è¡Œé¢„æµ‹ã€‚"""
        handler = self.active_model_handler
        if handler.scaler is None or not hasattr(handler.scaler, "mean_"):
            return {'message': "Scaler æœªåŠ è½½æˆ–æœªæ‹Ÿåˆã€‚"}
        try:
            features_df = self.prepare_features(data, symbol=symbol)
            if features_df.empty: return {'message': "ç‰¹å¾å‡†å¤‡å¤±è´¥ã€‚"}

            # --- å…³é”®ä¿®å¤ï¼šç›´æ¥å–æœ€åä¸€è¡Œï¼Œç„¶åæ£€æŸ¥ ---
            latest_features_row = features_df.iloc[-1:]
            if latest_features_row[self.required_features].isnull().values.any():
                return {'message': "æœ€æ–°çš„ç‰¹å¾æ•°æ®åŒ…å«æ— æ•ˆå€¼(NaN)ï¼Œæ— æ³•é¢„æµ‹ã€‚"}

            latest_features = latest_features_row[self.required_features].values

            features_scaled = handler.scaler.transform(latest_features)
            prediction = handler.model.predict(features_scaled)[0]
            probabilities = handler.model.predict_proba(features_scaled)[0]

            feature_importance = {}
            if hasattr(handler.model, 'feature_importances_'):
                feature_importance = dict(zip(self.required_features, handler.model.feature_importances_))

            return {
                'direction': int(prediction), 'confidence': float(max(probabilities)),
                'probability_up': float(probabilities[1]), 'probability_down': float(probabilities[0]),
                'feature_importance': feature_importance, 'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'model_used': self.current_model_name
            }
        except Exception as e:
            self.logger.error(f"RandomForest é¢„æµ‹æ—¶å‡ºé”™: {e}", exc_info=True)
            return {'message': f"RandomForest é¢„æµ‹å‡ºé”™: {e}"}

    def _predict_dl(self, data: pd.DataFrame, symbol: str, model_type: str) -> Optional[Dict]:
        """[æœ€ç»ˆä¿®å¤ç‰ˆ] é¢„æµ‹æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚"""
        try:
            handler = self.active_model_handler
            # vvvvvvvvvvvvvvvvvvvv START OF FIX vvvvvvvvvvvvvvvvvvvv
            # --- åœ¨æ–¹æ³•å¼€å¤´å°±è·å–æ‰€æœ‰éœ€è¦çš„è¶…å‚æ•° ---
            params = self.config.ML_HYPERPARAMETERS.get(model_type, {})
            lookback = params.get('lookback', 60)  # 60 æ˜¯é»˜è®¤å€¼

            # 1. åŠ è½½è®­ç»ƒæ—¶ä½¿ç”¨çš„ Scaler
            scaler_path = self.models_base_path / f"{Path(self.current_model_filename).stem}_scaler.joblib"
            scaler = load_ml_resource(scaler_path)
            if not scaler or not hasattr(scaler, "n_features_in_"):
                return {'message': "æ‰¾ä¸åˆ°æˆ–æœªè®­ç»ƒç”¨äºé¢„æµ‹çš„ Scaler æ–‡ä»¶ã€‚"}

            # 2. å‡†å¤‡é¢„æµ‹æ‰€éœ€çš„ç‰¹å¾ (ç°åœ¨ prepare_features æ€»æ˜¯è¿”å›å›ºå®šç»“æ„çš„ df)
            features_df = self.prepare_features(data, symbol=symbol)
            if len(features_df) < lookback:
                return {'message': f"å‡†å¤‡é¢„æµ‹æ•°æ®åè¡Œæ•°ä¸è¶³({len(features_df)})"}

            # vvvvvvvvvvvvvvvvvvvv START OF FIX vvvvvvvvvvvvvvvvvvvv
            # 3. ä½¿ç”¨ scaler ä¸­ä¿å­˜çš„ç‰¹å¾åæ¥ç¡®ä¿ä¸€è‡´æ€§
            training_feature_cols = getattr(scaler, 'feature_names_in_', None)
            if training_feature_cols is None:
                # å¦‚æœæ—§çš„ scaler æ²¡æœ‰ä¿å­˜ç‰¹å¾åï¼Œæˆ‘ä»¬è¿›è¡Œå›é€€
                self.logger.warning("Scaler does not contain feature names. Falling back to config.")
                training_feature_cols = self.base_feature_columns + self.text_feature_columns

            missing_cols = set(training_feature_cols) - set(features_df.columns)
            if missing_cols:
                return {'message': f"é¢„æµ‹æ•°æ®ä¸­ç¼ºå°‘åˆ—: {', '.join(missing_cols)}"}

            # 4. åªé€‰æ‹© scaler æœŸæœ›çš„åˆ—ï¼Œå¹¶å–æœ€å lookback è¡Œ
            latest_data_to_scale = features_df[training_feature_cols].iloc[-lookback:]
            # ^^^^^^^^^^^^^^^^^^^^ END OF FIX ^^^^^^^^^^^^^^^^^^^^

            # 5. åº”ç”¨ Scaler å¹¶å‡†å¤‡è¾“å…¥
            scaled_features_array = scaler.transform(latest_data_to_scale).astype(np.float32)
            X_pred = np.reshape(scaled_features_array, (1, lookback, scaled_features_array.shape[1]))

            if model_type == 'AlphaTransformer':
                predicted_alpha = handler.model.predict(X_pred, verbose=0)[0][0]
                # --- å…³é”®ä¿®å¤ï¼šè®©æ¨¡å‹æ€»æ˜¯ç»™å‡ºæ–¹å‘ ---
                direction = 1 if predicted_alpha > 0 else 0
                # --- ç»“æŸä¿®å¤ ---
                confidence = min(1.0, abs(predicted_alpha) / 0.05)
                return {'direction': direction, 'predicted_alpha': float(predicted_alpha),
                        'confidence': float(confidence), 'model_used': self.current_model_name}
            else:
                probability_up = handler.model.predict(X_pred, verbose=0)[0][0]
                prediction = 1 if probability_up > 0.5 else 0
                return {'direction': prediction, 'probability_up': float(probability_up),
                        'confidence': abs(probability_up - 0.5) * 2, 'model_used': self.current_model_name}
        except Exception as e:
            self.logger.error(f"{model_type} é¢„æµ‹æ—¶å‡ºé”™: {e}", exc_info=True)
            return {'message': f"{model_type} é¢„æµ‹å‡ºé”™: {e}"}

    def predict_for_backtest(self, features_df: pd.DataFrame, symbol: str) -> Optional[pd.Series]:
        """
        [æœ€ç»ˆä¿®å¤ç‰ˆ] ä¸ºå›æµ‹åˆ›å»ºç‹¬ç«‹çš„æ¨¡å‹é¢„æµ‹ï¼Œç¡®ä¿æ•°æ®æµç•…é€šã€‚
        """
        if not self.active_model_handler or not getattr(self.active_model_handler, 'model', None):
            self.logger.error("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•è¿›è¡Œæ‰¹é‡é¢„æµ‹")
            return None

        self.logger.info(f"å¼€å§‹æ‰¹é‡é¢„æµ‹ for {symbol} ({len(features_df)} è¡Œæ•°æ®)...")

        try:
            model_type = self._get_model_type(self.current_model_filename)
            self.logger.info(f"ä½¿ç”¨æ¨¡å‹ç±»å‹: {model_type}")

            if model_type in ['LSTM', 'AlphaTransformer']:
                return self._predict_dl_batch(features_df, symbol, model_type)
            elif model_type == 'RandomForest':
                return self._predict_sklearn_batch(features_df, symbol)
            else:
                self.logger.error(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")
                return None

        except Exception as e:
            self.logger.error(f"æ‰¹é‡é¢„æµ‹å¤±è´¥: {e}", exc_info=True)
            return None

    def _predict_dl_batch(self, features_df: pd.DataFrame, symbol: str, model_type: str) -> Optional[pd.Series]:
        """æ·±åº¦å­¦ä¹ æ¨¡å‹çš„æ‰¹é‡é¢„æµ‹"""
        params = self.config.ML_HYPERPARAMETERS.get(model_type, {})
        lookback = params.get('lookback', 60)

        # 1. åŠ è½½ scaler
        scaler_path = self.models_base_path / f"{Path(self.current_model_filename).stem}_scaler.joblib"
        scaler = load_ml_resource(scaler_path)
        if not scaler:
            self.logger.error(f"æ— æ³•åŠ è½½ scaler: {scaler_path}")
            return None

        # 2. è·å–è®­ç»ƒæ—¶çš„ç‰¹å¾åˆ—
        training_feature_cols = getattr(scaler, 'feature_names_in_', None)
        if training_feature_cols is None:
            # ä¿®å¤ï¼šå®šä¹‰å®Œæ•´çš„ç‰¹å¾åˆ—è¡¨
            technical_features = [
                'ma5', 'ma10', 'ma20', 'ma30', 'ma60',
                'vol_20', 'vol_60', 'vol_chg_5',
                'mom_20', 'mom_60',
                'bollinger_upper', 'bollinger_middle', 'bollinger_lower',
                'signal', 'hist'
            ]
            training_feature_cols = (
                    self.base_feature_columns +
                    technical_features +
                    self.text_feature_columns
            )
            self.logger.warning("Scalerä¸­æ²¡æœ‰feature_names_in_ï¼Œä½¿ç”¨å®Œæ•´ç‰¹å¾åˆ—é…ç½®")

        self.logger.info(f"ğŸ“‹ éœ€è¦çš„ç‰¹å¾åˆ—æ•°é‡: {len(training_feature_cols)}")

        # 3. æ£€æŸ¥å¹¶è®¡ç®—ç¼ºå¤±çš„æŠ€æœ¯æŒ‡æ ‡
        missing_tech_features = []
        for col in training_feature_cols:
            if col not in features_df.columns and col in [
                'ma5', 'ma10', 'ma20', 'ma30', 'ma60',
                'vol_20', 'vol_60', 'vol_chg_5',
                'mom_20', 'mom_60',
                'bollinger_upper', 'bollinger_middle', 'bollinger_lower',
                'signal', 'hist'
            ]:
                missing_tech_features.append(col)

        if missing_tech_features:
            self.logger.info(f"ğŸ”§ éœ€è¦è®¡ç®—ç¼ºå¤±çš„æŠ€æœ¯æŒ‡æ ‡: {missing_tech_features}")
            try:
                features_df = self._add_missing_technical_features(features_df, missing_tech_features)
            except Exception as e:
                self.logger.error(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¤±è´¥: {e}")
                return None

        # 4. éªŒè¯ç‰¹å¾åˆ—
        missing_cols = set(training_feature_cols) - set(features_df.columns)
        if missing_cols:
            self.logger.error(f"âŒ ä»ç„¶ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}")
            return None

        # 5. å‡†å¤‡æ•°æ®
        features_clean = features_df[training_feature_cols].copy()

        # å¤„ç†æ— æ•ˆå€¼
        features_clean = features_clean.replace([np.inf, -np.inf], np.nan)
        features_clean = features_clean.ffill().bfill().fillna(0.0)

        X_unscaled = features_clean.values.astype(np.float32)

        if len(X_unscaled) < lookback:
            self.logger.error(f"æ•°æ®é•¿åº¦ä¸è¶³: {len(X_unscaled)} < {lookback}")
            return None

        # 6. æ•°æ®ç¼©æ”¾
        X_scaled = scaler.transform(X_unscaled)

        # 7. åˆ›å»ºåºåˆ—æ•°æ®
        X_sequences = []
        valid_indices = []

        for i in range(lookback - 1, len(X_scaled)):
            X_sequences.append(X_scaled[i - lookback + 1:i + 1])
            valid_indices.append(features_df.index[i])

        if not X_sequences:
            self.logger.error("æ— æ³•åˆ›å»ºæœ‰æ•ˆçš„åºåˆ—æ•°æ®")
            return None

        X_sequences = np.array(X_sequences)
        self.logger.info(f"åºåˆ—æ•°æ®å½¢çŠ¶: {X_sequences.shape}")

        # 8. æ‰¹é‡é¢„æµ‹
        try:
            batch_size = min(32, len(X_sequences))
            predictions = []

            num_batches = (len(X_sequences) + batch_size - 1) // batch_size

            for i in range(num_batches):
                start_idx = i * batch_size
                end_idx = min((i + 1) * batch_size, len(X_sequences))
                batch_data = X_sequences[start_idx:end_idx]

                batch_pred = self.active_model_handler.model.predict(
                    batch_data, batch_size=len(batch_data), verbose=0
                )
                predictions.extend(batch_pred.flatten())

            predictions = np.array(predictions)

        except Exception as e:
            self.logger.error(f"æ‰¹é‡é¢„æµ‹æ‰§è¡Œå¤±è´¥: {e}")
            return None

        # 9. è½¬æ¢ä¸º alpha åˆ†æ•°
        if model_type == 'AlphaTransformer':
            alpha_scores = predictions
        else:
            alpha_scores = predictions - 0.5

        # 10. åˆ›å»ºç»“æœ Series
        result_series = pd.Series(alpha_scores, index=valid_indices, name='alpha_score')

        self.logger.info(f"æ‰¹é‡é¢„æµ‹å®Œæˆï¼Œè¿”å› {len(result_series)} ä¸ªé¢„æµ‹å€¼")
        return result_series

    def _add_missing_technical_features(self, df: pd.DataFrame, missing_features: list) -> pd.DataFrame:
        """ä¸ºç¼ºå¤±çš„æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾æ·»åŠ è®¡ç®—"""

        result_df = df.copy()

        # éœ€è¦ä»·æ ¼æ•°æ®åˆ—
        required_cols = ['close', 'high', 'low', 'volume']
        missing_price_cols = [col for col in required_cols if col not in df.columns]
        if missing_price_cols:
            raise ValueError(f"ç¼ºå°‘è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ‰€éœ€çš„ä»·æ ¼æ•°æ®: {missing_price_cols}")

        for feature in missing_features:
            try:
                if feature.startswith('ma'):
                    # ç§»åŠ¨å¹³å‡çº¿
                    period = int(feature[2:])  # ma20 -> 20
                    result_df[feature] = df['close'].rolling(window=period).mean()

                elif feature.startswith('vol_'):
                    # æ³¢åŠ¨ç‡
                    if feature == 'vol_chg_5':
                        result_df[feature] = df['volume'].pct_change(5)
                    else:
                        period = int(feature[4:])  # vol_60 -> 60
                        result_df[feature] = df['close'].rolling(window=period).std()

                elif feature.startswith('mom_'):
                    # åŠ¨é‡æŒ‡æ ‡
                    period = int(feature[4:])  # mom_60 -> 60
                    result_df[feature] = df['close'].pct_change(period)

                elif feature.startswith('bollinger_'):
                    # å¸ƒæ—å¸¦
                    ma20 = df['close'].rolling(window=20).mean()
                    std20 = df['close'].rolling(window=20).std()
                    if feature == 'bollinger_upper':
                        result_df[feature] = ma20 + 2 * std20
                    elif feature == 'bollinger_lower':
                        result_df[feature] = ma20 - 2 * std20
                    elif feature == 'bollinger_middle':
                        result_df[feature] = ma20

                elif feature == 'signal':
                    # ä¿¡å·ç‰¹å¾ï¼ˆç®€å•å®ç°ï¼‰
                    result_df[feature] = (df['close'] > df['close'].rolling(5).mean()).astype(int)

                elif feature == 'hist':
                    # å†å²ç‰¹å¾ï¼ˆå¯ä»¥æ˜¯ä»»ä½•å†å²ç»Ÿè®¡ï¼‰
                    result_df[feature] = df['close'].rolling(window=20).apply(lambda x: len(x))

            except Exception as e:
                self.logger.warning(f"è®¡ç®—ç‰¹å¾ {feature} å¤±è´¥: {e}")
                # å¡«å……é»˜è®¤å€¼
                result_df[feature] = 0.0

        # å¡«å…… NaN å€¼
        result_df = result_df.ffill().bfill().fillna(0.0)

        self.logger.info(f"âœ… æˆåŠŸæ·»åŠ  {len(missing_features)} ä¸ªæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾")
        return result_df

    def _predict_sklearn_batch(self, features_df: pd.DataFrame, symbol: str) -> Optional[pd.Series]:
        """Sklearnæ¨¡å‹çš„æ‰¹é‡é¢„æµ‹"""
        handler = self.active_model_handler

        # 1. éªŒè¯ç‰¹å¾åˆ—
        missing_cols = set(self.required_features) - set(features_df.columns)
        if missing_cols:
            self.logger.error(f"ç¼ºå°‘ç‰¹å¾åˆ—: {missing_cols}")
            return None

        # 2. å‡†å¤‡æ•°æ®
        features_clean = features_df[self.required_features].copy()
        features_clean = features_clean.replace([np.inf, -np.inf], np.nan)
        features_clean = features_clean.ffill().bfill().fillna(0.0)

        X_unscaled = features_clean.values
        X_scaled = handler.scaler.transform(X_unscaled)

        # 3. æ‰¹é‡é¢„æµ‹
        probabilities = handler.model.predict_proba(X_scaled)
        alpha_scores = probabilities[:, 1] - 0.5

        # 4. è¿”å›ç»“æœ
        result_series = pd.Series(alpha_scores, index=features_df.index, name='alpha_score')
        return result_series

    def get_feature_importance(self) -> Dict[str, float]:
        if isinstance(self.active_model_handler,
                      SklearnHandler) and self.active_model_handler.model is not None and hasattr(
                self.active_model_handler.model, 'feature_importances_'):
            try:
                check_is_fitted(self.active_model_handler.model)
                importances = self.active_model_handler.model.feature_importances_
                return dict(sorted(dict(zip(self.required_features, importances)).items(), key=lambda item: item[1],
                                   reverse=True))
            except Exception as e:
                self.logger.error(f"è·å–ç‰¹å¾é‡è¦æ€§å¤±è´¥: {e}");
                return {}
        return {}

    def _save_model(self) -> bool:
        """ä¿å­˜å½“å‰æ´»åŠ¨çš„ Sklearn æ¨¡å‹å’Œ Scaler"""
        handler = self.active_model_handler
        if not (isinstance(handler,
                           SklearnHandler) and self.current_model_filename and handler.model and handler.scaler):
            self.logger.error("æ— æ³•ä¿å­˜ Sklearn æ¨¡å‹ï¼šå¤„ç†å™¨ã€æ–‡ä»¶åã€æ¨¡å‹æˆ– Scaler ç¼ºå¤±ã€‚")
            return False
        try:
            model_path = self.models_base_path / self.current_model_filename
            scaler_path = self.models_base_path / f"{Path(self.current_model_filename).stem}_scaler.joblib"
            joblib.dump(handler.model, model_path)
            joblib.dump(handler.scaler, scaler_path)
            self.logger.info(f"Sklearn æ¨¡å‹ '{self.current_model_name}' å’Œ Scaler å·²ä¿å­˜ã€‚")
            clear_load_ml_resource_cache()
            return True
        except Exception as e:
            self.logger.error(f"ä¿å­˜ Sklearn æ¨¡å‹å¤±è´¥: {e}", exc_info=True)
            return False

    #     def _calculate_base_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        [æ–°å¢] ä¸“é—¨ç”¨äºè®¡ç®—æ‰€æœ‰åŸºç¡€æŠ€æœ¯æŒ‡æ ‡å’Œå¢å¼ºç‰¹å¾çš„ç§æœ‰æ–¹æ³•ã€‚
        """
        #         self.logger.debug(f"Calculating base features for {len(df)} rows...")
        #         min_periods_req = 1

        # --- åŸºç¡€æŠ€æœ¯æŒ‡æ ‡ ---
        for period in [5, 10, 20, 30, 60]:
            df[f'ma{period}'] = df['close'].rolling(window=period, min_periods=min_periods_req).mean()

        delta = df['close'].diff()
        gain = delta.where(delta > 0, 0.0).rolling(window=14, min_periods=min_periods_req).mean()
        loss = (-delta.where(delta < 0, 0.0)).rolling(window=14, min_periods=min_periods_req).mean()
        rs = gain / loss.replace(0, 1e-9)
        df['rsi'] = 100.0 - (100.0 / (1.0 + rs))
        df['rsi'] = df['rsi'].fillna(50)

        exp1 = df['close'].ewm(span=12, adjust=False).mean()
        exp2 = df['close'].ewm(span=26, adjust=False).mean()
        df['macd'] = exp1 - exp2
        df['signal'] = df['macd'].ewm(span=9, adjust=False).mean()
        df['hist'] = df['macd'] - df['signal']

        df['bollinger_middle'] = df['close'].rolling(window=20, min_periods=min_periods_req).mean()
        std = df['close'].rolling(window=20, min_periods=min_periods_req).std().fillna(0)
        df['bollinger_upper'] = df['bollinger_middle'] + (2 * std)
        df['bollinger_lower'] = df['bollinger_middle'] - (2 * std)

        high_low = df['high'] - df['low']
        high_close = abs(df['high'] - df['close'].shift())
        low_close = abs(df['low'] - df['close'].shift())
        ranges = pd.concat([high_low, high_close, low_close], axis=1).fillna(0)
        true_range = ranges.max(axis=1)
        df['atr'] = true_range.rolling(14, min_periods=min_periods_req).mean()

        plus_dm = (df['high'] - df['high'].shift(1)).fillna(0)
        minus_dm = (df['low'].shift(1) - df['low']).fillna(0)
        plus_dm[(plus_dm < 0) | (plus_dm < minus_dm)] = 0.0
        minus_dm[(minus_dm < 0) | (minus_dm < plus_dm)] = 0.0
        atr14 = df['atr']
        plus_di = 100 * (plus_dm.ewm(alpha=1 / 14, adjust=False).mean() / atr14.replace(0, 1e-9))
        minus_di = 100 * (minus_dm.ewm(alpha=1 / 14, adjust=False).mean() / atr14.replace(0, 1e-9))
        dx_numerator = abs(plus_di - minus_di)
        dx_denominator = (plus_di + minus_di).replace(0, 1e-9)
        dx = 100 * (dx_numerator / dx_denominator)
        df['adx'] = dx.ewm(alpha=1 / 14, adjust=False).mean()

        tp = (df['high'] + df['low'] + df['close']) / 3
        sma_tp = tp.rolling(20, min_periods=min_periods_req).mean()
        try:
            mean_dev = tp.rolling(20, min_periods=min_periods_req).apply(lambda x: pd.Series(x).mad(), raw=True)
        except AttributeError:
            mean_dev = abs(tp - sma_tp).rolling(20, min_periods=min_periods_req).mean()
        df['cci'] = (tp - sma_tp) / (0.015 * mean_dev.replace(0, 1e-9).fillna(1e-9))

        # --- å¢å¼ºç‰¹å¾ ---
        df['mom_20'] = df['close'] / df['close'].shift(20).replace(0, 1e-9)
        df['mom_60'] = df['close'] / df['close'].shift(60).replace(0, 1e-9)

        daily_returns = df['close'].pct_change()
        df['vol_20'] = daily_returns.rolling(window=20, min_periods=min_periods_req).std() * np.sqrt(252)
        df['vol_60'] = daily_returns.rolling(window=60, min_periods=min_periods_req).std() * np.sqrt(252)

        if 'volume' in df.columns and not df['volume'].empty:
            vol_ma_5 = df['volume'].rolling(window=5, min_periods=min_periods_req).mean()
            vol_ma_60 = df['volume'].rolling(window=60, min_periods=min_periods_req).mean()
            df['vol_chg_5'] = vol_ma_5 / vol_ma_60.replace(0, 1e-9)
        else:
            df['vol_chg_5'] = 1.0
        df.replace([np.inf, -np.inf], np.nan, inplace=True)

        return df

    def prepare_features(self, data: pd.DataFrame, symbol: str) -> pd.DataFrame:
        # åœ¨æ–¹æ³•å¼€å¤´æ·»åŠ å¼€å…³çŠ¶æ€æ£€æŸ¥
        self.logger.info(f"=== ç‰¹å¾å‡†å¤‡å¼€å§‹ for {symbol} ===")
        self.logger.info(f"TextFeatureExtractor å¯ç”¨çŠ¶æ€: {self.text_feature_extractor is not None}")
        if self.text_feature_extractor:
            self.logger.info(f"TextFeatureExtractor.is_available: {self.text_feature_extractor.is_available}")
        """
        [ç»ˆæä¿®å¤ç‰ˆ] å‡†å¤‡æ‰€æœ‰ç‰¹å¾ï¼ŒåŒ…å«è¯¦ç»†è°ƒè¯•ä¿¡æ¯ã€‚
        """
        if data is None or data.empty:
            self.logger.error(f"prepare_features received empty or None data for {symbol}.")
            return pd.DataFrame()

        self.logger.info(f"=== å¼€å§‹ç‰¹å¾å‡†å¤‡ for {symbol} ===")
        self.logger.info(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {data.shape}")
        self.logger.info(f"è¾“å…¥åˆ—å: {list(data.columns)}")

        try:
            # æ­¥éª¤1: å¤åˆ¶æ•°æ®å¹¶æ ‡å‡†åŒ–åˆ—å
            df = data.copy()
            original_columns = list(df.columns)
            df.columns = [str(col).lower().replace(' ', '_') for col in df.columns]
            self.logger.info(f"æ ‡å‡†åŒ–ååˆ—å: {list(df.columns)}")

            # æ­¥éª¤2: éªŒè¯åŸºç¡€æ•°æ®åˆ—
            required_raw_cols = ['open', 'high', 'low', 'close', 'volume']
            missing_raw_cols = set(required_raw_cols) - set(df.columns)
            if missing_raw_cols:
                self.logger.error(f"ç¼ºå°‘åŸºç¡€æ•°æ®åˆ—: {missing_raw_cols}")
                return pd.DataFrame()

            # æ­¥éª¤3: æ£€æŸ¥æ•°æ®è´¨é‡
            self.logger.info(f"æ•°æ®æ—¥æœŸèŒƒå›´: {df.index.min()} åˆ° {df.index.max()}")
            self.logger.info(
                f"Closeåˆ—ç»Ÿè®¡: min={df['close'].min():.2f}, max={df['close'].max():.2f}, mean={df['close'].mean():.2f}")

            # æ­¥éª¤4: æ£€æŸ¥æ˜¯å¦å·²æœ‰æŠ€æœ¯æŒ‡æ ‡
            expected_technical_cols = ['ma_5', 'ma_10', 'ma_20', 'rsi', 'macd']
            existing_technical_cols = [col for col in expected_technical_cols if col in df.columns]

            if len(existing_technical_cols) >= 3:
                self.logger.info(f"æ£€æµ‹åˆ°ç°æœ‰æŠ€æœ¯æŒ‡æ ‡: {existing_technical_cols}")
                has_technical = True
            else:
                self.logger.info("æœªæ£€æµ‹åˆ°æŠ€æœ¯æŒ‡æ ‡ï¼Œéœ€è¦è®¡ç®—")
                has_technical = False

            # æ­¥éª¤5: è®¡ç®—æˆ–éªŒè¯æŠ€æœ¯æŒ‡æ ‡
            if not has_technical:
                self.logger.info("å¼€å§‹è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
                try:
                    from core.analysis.technical import TechnicalAnalyzer
                    technical_analyzer = TechnicalAnalyzer(self.config)
                    df_with_tech = technical_analyzer.analyze(df)

                    if df_with_tech.empty:
                        self.logger.error("TechnicalAnalyzer è¿”å›ç©º DataFrame")
                        return pd.DataFrame()

                    df = df_with_tech
                    self.logger.info(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆï¼Œæ–°çš„åˆ—æ•°: {len(df.columns)}")
                    self.logger.info(f"æ–°å¢çš„åˆ—: {set(df.columns) - set(original_columns)}")

                except Exception as e:
                    self.logger.error(f"æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}", exc_info=True)
                    return pd.DataFrame()

            # æ­¥éª¤6: æ˜ å°„åˆ—ååˆ°é…ç½®æœŸæœ›çš„æ ¼å¼
            self.logger.info("å¼€å§‹åˆ—åæ˜ å°„...")
            column_mapping = {
                # TechnicalAnalyzer è¾“å‡º -> config.py æœŸæœ›
                'ma_5': 'ma5', 'ma_10': 'ma10', 'ma_20': 'ma20', 'ma_30': 'ma30', 'ma_60': 'ma60',
                'macd_signal': 'signal', 'macd_diff': 'hist',
                'bb_upper': 'bollinger_upper', 'bb_middle': 'bollinger_middle', 'bb_lower': 'bollinger_lower'
            }

            for tech_name, config_name in column_mapping.items():
                if tech_name in df.columns and config_name not in df.columns:
                    df[config_name] = df[tech_name]
                    self.logger.debug(f"æ˜ å°„: {tech_name} -> {config_name}")

            # æ­¥éª¤7: è®¡ç®—ç¼ºå¤±çš„å¢å¼ºç‰¹å¾
            self.logger.info("è®¡ç®—å¢å¼ºç‰¹å¾...")

            # åŠ¨é‡ç‰¹å¾
            if 'mom_20' not in df.columns:
                df['mom_20'] = df['close'] / df['close'].shift(20).replace(0, 1e-9)
                self.logger.debug("è®¡ç®— mom_20")

            if 'mom_60' not in df.columns:
                df['mom_60'] = df['close'] / df['close'].shift(60).replace(0, 1e-9)
                self.logger.debug("è®¡ç®— mom_60")

            # æ³¢åŠ¨ç‡ç‰¹å¾
            if 'vol_20' not in df.columns:
                daily_returns = df['close'].pct_change()
                df['vol_20'] = daily_returns.rolling(window=20, min_periods=1).std() * np.sqrt(252)
                self.logger.debug("è®¡ç®— vol_20")

            if 'vol_60' not in df.columns:
                daily_returns = df['close'].pct_change()
                df['vol_60'] = daily_returns.rolling(window=60, min_periods=1).std() * np.sqrt(252)
                self.logger.debug("è®¡ç®— vol_60")

            # æˆäº¤é‡ç‰¹å¾
            if 'vol_chg_5' not in df.columns:
                if 'volume' in df.columns and not df['volume'].isna().all():
                    vol_ma_5 = df['volume'].rolling(window=5, min_periods=1).mean()
                    vol_ma_60 = df['volume'].rolling(window=60, min_periods=1).mean()
                    df['vol_chg_5'] = vol_ma_5 / vol_ma_60.replace(0, 1e-9)
                    self.logger.debug("è®¡ç®— vol_chg_5")
                else:
                    df['vol_chg_5'] = 1.0
                    self.logger.debug("volumeåˆ—æ— æ•ˆï¼Œvol_chg_5è®¾ä¸º1.0")

            # æ­¥éª¤8: åˆå§‹åŒ–æ–‡æœ¬ç‰¹å¾
            self.logger.info("åˆå§‹åŒ–æ–‡æœ¬ç‰¹å¾åˆ—...")
            for col in self.text_feature_columns:
                if col not in df.columns:
                    df[col] = 0.0
                    self.logger.debug(f"åˆå§‹åŒ–æ–‡æœ¬ç‰¹å¾: {col} = 0.0")

            # æ­¥éª¤9: æ™ºèƒ½è·å–æ–‡æœ¬ç‰¹å¾ï¼ˆæ”¯æŒå¼€å…³æ§åˆ¶ï¼‰
            text_features_obtained = False

            if self.text_feature_extractor and self.text_feature_extractor.is_available:
                self.logger.info(f"TextFeatureExtractor å¯ç”¨ï¼Œå°è¯•ä¸º {symbol} è·å–æ–‡æœ¬ç‰¹å¾...")

                try:
                    # è°ƒç”¨ä¿®å¤åçš„æ‰¹é‡æ–‡æœ¬ç‰¹å¾è·å–æ–¹æ³•
                    text_features_df = self.text_feature_extractor.get_and_extract_features_for_backtest(symbol,
                                                                                                         df.index)

                    if text_features_df is not None and not text_features_df.empty:
                        self.logger.info(f"æˆåŠŸè·å–æ–‡æœ¬ç‰¹å¾ï¼Œå½¢çŠ¶: {text_features_df.shape}")

                        # ç¡®ä¿ç´¢å¼•åç§°ä¸€è‡´
                        if df.index.name != text_features_df.index.name:
                            text_features_df.index.name = df.index.name

                        # åªåˆå¹¶æˆ‘ä»¬éœ€è¦çš„æ–‡æœ¬ç‰¹å¾åˆ—
                        text_cols_to_merge = [col for col in self.text_feature_columns if
                                              col in text_features_df.columns]

                        if text_cols_to_merge:
                            # ä½¿ç”¨ left join ç¡®ä¿ä¿æŒåŸæœ‰çš„æ•°æ®è¡Œæ•°
                            df = df.join(text_features_df[text_cols_to_merge], how='left')

                            # å¡«å……å¯èƒ½çš„ NaN å€¼
                            for col in text_cols_to_merge:
                                df[col] = df[col].fillna(0.0)

                            self.logger.info(f"æˆåŠŸåˆå¹¶ {len(text_cols_to_merge)} ä¸ªæ–‡æœ¬ç‰¹å¾åˆ—: {text_cols_to_merge}")
                            text_features_obtained = True
                        else:
                            self.logger.warning("æ–‡æœ¬ç‰¹å¾DataFrameä¸­æ²¡æœ‰æ‰¾åˆ°æœŸæœ›çš„åˆ—")
                    else:
                        self.logger.warning("æ–‡æœ¬ç‰¹å¾è·å–è¿”å›äº†ç©ºç»“æœ")

                except Exception as e:
                    self.logger.error(f"æ–‡æœ¬ç‰¹å¾è·å–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                    self.logger.info("å°†ç»§ç»­ä½¿ç”¨é»˜è®¤çš„æ–‡æœ¬ç‰¹å¾å€¼")
            else:
                self.logger.info("TextFeatureExtractor ä¸å¯ç”¨ï¼Œè·³è¿‡æ–‡æœ¬ç‰¹å¾è·å–")

            # ç¡®ä¿æ‰€æœ‰æ–‡æœ¬ç‰¹å¾åˆ—éƒ½å­˜åœ¨ä¸”æœ‰æœ‰æ•ˆå€¼
            for col in self.text_feature_columns:
                if col not in df.columns:
                    df[col] = 0.0
                    self.logger.debug(f"è¡¥å……ç¼ºå¤±çš„æ–‡æœ¬ç‰¹å¾åˆ—: {col}")
                elif df[col].isna().any():
                    df[col] = df[col].fillna(0.0)
                    self.logger.debug(f"å¡«å……æ–‡æœ¬ç‰¹å¾åˆ—ä¸­çš„NaN: {col}")

            if text_features_obtained:
                self.logger.info("âœ… æ–‡æœ¬ç‰¹å¾è·å–æˆåŠŸ")
            else:
                self.logger.info("âš ï¸ ä½¿ç”¨é»˜è®¤æ–‡æœ¬ç‰¹å¾å€¼ï¼ˆå…¨ä¸º0ï¼‰")

            # æ­¥éª¤10: æ•°æ®æ¸…ç†
            self.logger.info("å¼€å§‹æ•°æ®æ¸…ç†...")
            initial_shape = df.shape

            # æ›¿æ¢æ— ç©·å€¼
            df.replace([np.inf, -np.inf], np.nan, inplace=True)
            inf_count = df.isnull().sum().sum() - initial_shape[0] * initial_shape[1] + df.count().sum()
            if inf_count > 0:
                self.logger.info(f"æ›¿æ¢äº† {inf_count} ä¸ªæ— ç©·å€¼")

            # å¡«å……ç¼ºå¤±å€¼
            df = df.ffill().bfill().fillna(0)

            # æ­¥éª¤11: æœ€ç»ˆåˆ—é€‰æ‹©å’ŒéªŒè¯
            self.logger.info("æœ€ç»ˆåˆ—é€‰æ‹©...")
            required_base_features = [col for col in self.base_feature_columns if col != 'close']  # æš‚æ—¶æ’é™¤closeè¿›è¡Œæ£€æŸ¥
            required_text_features = self.text_feature_columns[:]
            all_required_features = required_base_features + required_text_features + ['close']

            self.logger.info(f"éœ€è¦çš„ç‰¹å¾åˆ—: {all_required_features}")

            available_features = [col for col in all_required_features if col in df.columns]
            missing_features = set(all_required_features) - set(available_features)

            if missing_features:
                self.logger.warning(f"ç¼ºå¤±çš„ç‰¹å¾: {missing_features}")
                # å¯¹äºç¼ºå¤±çš„éå…³é”®ç‰¹å¾ï¼Œç”¨é»˜è®¤å€¼å¡«å……
                for missing_col in missing_features:
                    if missing_col != 'close':  # closeæ˜¯å¿…éœ€çš„
                        df[missing_col] = 0.0
                        self.logger.info(f"ç”¨é»˜è®¤å€¼å¡«å……ç¼ºå¤±ç‰¹å¾: {missing_col}")
                        available_features.append(missing_col)

            # æ£€æŸ¥closeåˆ—
            if 'close' not in df.columns:
                self.logger.error("å…³é”®åˆ— 'close' ç¼ºå¤±ï¼Œæ— æ³•ç»§ç»­")
                return pd.DataFrame()

            # æœ€ç»ˆæ•°æ®æ¡†
            final_df = df[available_features].copy()

            # æ­¥éª¤12: æœ€ç»ˆéªŒè¯
            self.logger.info(f"æœ€ç»ˆæ•°æ®å½¢çŠ¶: {final_df.shape}")
            self.logger.info(f"æœ€ç»ˆåˆ—å: {list(final_df.columns)}")

            # æ£€æŸ¥æ˜¯å¦æœ‰ç©ºæ•°æ®
            if final_df.empty:
                self.logger.error("æœ€ç»ˆæ•°æ®æ¡†ä¸ºç©º!")
                return pd.DataFrame()

            # æ£€æŸ¥æ•°æ®è´¨é‡
            null_counts = final_df.isnull().sum()
            if null_counts.sum() > 0:
                self.logger.warning(f"æœ€ç»ˆæ•°æ®ä¸­ä»æœ‰ç©ºå€¼: {null_counts[null_counts > 0].to_dict()}")

            # æ£€æŸ¥æ•°å€¼èŒƒå›´
            numeric_cols = final_df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                col_data = final_df[col]
                if col_data.isna().all():
                    self.logger.warning(f"åˆ— {col} å…¨ä¸ºNaN")
                elif (col_data == 0).all():
                    self.logger.warning(f"åˆ— {col} å…¨ä¸º0")

            self.logger.info("=== ç‰¹å¾å‡†å¤‡å®Œæˆ ===")
            return final_df

        except Exception as e:
            self.logger.error(f"ç‰¹å¾å‡†å¤‡è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}", exc_info=True)
            return pd.DataFrame()

    def validate_features(self, data: pd.DataFrame) -> bool:
        """Validates if the DataFrame contains the required features and no NaNs."""
        if not self.is_available: return False
        if data is None or data.empty: return False
        if not self.required_features: return False  # Need features defined

        try:
            missing = set(self.required_features) - set(data.columns)
            if missing:
                self.logger.error(f"Feature validation failed: Missing columns: {missing}")
                return False
            if data[self.required_features].isnull().values.any():
                nan_cols = data.columns[data[self.required_features].isnull().any()].tolist()
                self.logger.error(f"Feature validation failed: NaN values found in columns: {nan_cols}")
                return False
            return True
        except Exception as e:
            self.logger.error(f"Error during feature validation: {e}", exc_info=True)
            return False